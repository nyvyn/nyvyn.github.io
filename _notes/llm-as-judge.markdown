---
layout: note
title: LLM as judge
description: Best practices when using LLMs as evaluators
date: 2025-1-3
status: Sprouting
---

# Introduction

Large Language Models (LLMs) can serve as effective evaluators or "judges" for various tasks, from assessing writing
quality to reviewing code. When using LLMs as judges, they analyze inputs against specific criteria and provide
structured feedback or scores.

This approach offers several advantages:

- Consistency in evaluation across multiple submissions
- Scalability for large-scale assessment tasks
- Ability to provide detailed, multi-faceted feedback
- Quick turnaround compared to human evaluation

However, it's important to recognize their limitations and establish clear evaluation frameworks to ensure reliable
results.
